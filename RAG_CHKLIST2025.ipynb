{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2946a41-71d4-457f-9269-991fd81f2234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF for PDF handling\n",
    "from openpyxl import load_workbook\n",
    "from docx import Document as DocxDocument\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from uuid import uuid4  # Import uuid4 for unique collection names\n",
    "from email import policy\n",
    "from email.parser import BytesParser  # For parsing .eml files\n",
    "\n",
    "# Path to your logo\n",
    "logo_path = r\"C:\\Users\\M\\Desktop\\RAKBANK.png\"\n",
    "\n",
    "# Declare vector_db globally\n",
    "vector_db = None\n",
    "\n",
    "# Function to perform OCR on the image and extract text\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(img)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the image: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract text from a PDF, including form fields\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    form_text = \"\"\n",
    "\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "        # Extract plain text from the pages\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "\n",
    "            # Extract form fields and their contents\n",
    "            fields = page.widgets()\n",
    "            if fields:  # Check if any widgets are found\n",
    "                for field in fields:\n",
    "                    field_name = field.field_name\n",
    "                    field_value = field.field_value  # Use field_value to get the filled text\n",
    "                    if field_value:\n",
    "                        form_text += f\"{field_name}: {field_value}\\n\"\n",
    "                    else:\n",
    "                        form_text += f\"{field_name}: [Empty]\\n\"\n",
    "\n",
    "        pdf_document.close()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the PDF: {e}\")\n",
    "\n",
    "    return text + \"\\n\" + form_text\n",
    "\n",
    "# Function to extract text from an EXCEL file\n",
    "def extract_text_from_excel(excel_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        wb = load_workbook(excel_path, data_only=True)\n",
    "        for sheet in wb:\n",
    "            for row in sheet.iter_rows(values_only=True):\n",
    "                row_text = [str(cell) if cell is not None else '' for cell in row]\n",
    "                text += ' '.join(row_text) + '\\n'\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the EXCEL file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract text from a WORD file\n",
    "def extract_text_from_word(word_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = DocxDocument(word_path)\n",
    "        for para in doc.paragraphs:\n",
    "            text += para.text + '\\n'\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the WORD file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract text from an EML file\n",
    "def extract_text_from_eml(eml_path):\n",
    "    try:\n",
    "        with open(eml_path, 'rb') as fp:\n",
    "            msg = BytesParser(policy=policy.default).parse(fp)\n",
    "        # Extract text content\n",
    "        text_parts = []\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                text_parts.append(part.get_payload(decode=True).decode('utf-8', errors='replace'))\n",
    "            elif part.get_content_type() == 'text/html':\n",
    "                # Optionally, extract text from HTML parts\n",
    "                html_content = part.get_payload(decode=True).decode('utf-8', errors='replace')\n",
    "                soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "                text = soup.get_text(separator=' ')\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                text_parts.append(text)\n",
    "        text = '\\n'.join(text_parts)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the EML file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract text from a website URL\n",
    "def extract_text_from_website(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                \"(KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the website. Status code: {response.status_code}\")\n",
    "            return \"\"\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Remove script and style elements\n",
    "        for script_or_style in soup([\"script\", \"style\"]):\n",
    "            script_or_style.decompose()\n",
    "\n",
    "        # Get text\n",
    "        text = soup.get_text(separator=' ')\n",
    "\n",
    "        # Remove multiple spaces and newlines\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the website: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Define a simple Document class\n",
    "class Document:\n",
    "    def __init__(self, text, metadata=None):\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "# Query prompt template\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "different versions of the given user question to retrieve relevant documents from\n",
    "a vector database. By generating multiple perspectives on the user question, your\n",
    "goal is to help the user overcome some of the limitations of the distance-based\n",
    "similarity search. Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Function to process multiple documents, including websites\n",
    "def process_documents(files, urls, checklist_path):\n",
    "    # Initialize the documents list and print debug info\n",
    "    print(\"Initializing document processing...\")\n",
    "    documents = []  # Ensuring the initialization of documents\n",
    "\n",
    "    # Process uploaded files\n",
    "    for file in files:  # Loop through each file\n",
    "        print(f\"File uploaded: {file.name}\")\n",
    "        if file.name.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            print(f\"Processing image: {file.name}\")\n",
    "            image_text = extract_text_from_image(file.name)\n",
    "            if image_text:\n",
    "                print(f\"Extracted text from image: {image_text[:100]}...\")  # Debug log (first 100 chars)\n",
    "                documents.append(Document(image_text, metadata={\"source\": file.name}))\n",
    "            else:\n",
    "                print(f\"No text extracted from the image: {file.name}\")\n",
    "        elif file.name.endswith(\".pdf\"):\n",
    "            print(f\"Processing PDF: {file.name}\")\n",
    "            pdf_text = extract_text_from_pdf(file.name)\n",
    "            if pdf_text:\n",
    "                print(f\"Extracted text from PDF: {pdf_text[:100]}...\")  # Debug log (first 100 chars)\n",
    "                documents.append(Document(pdf_text, metadata={\"source\": file.name}))\n",
    "            else:\n",
    "                print(f\"No text extracted from the PDF: {file.name}\")\n",
    "        elif file.name.endswith((\".xlsx\", \".xls\")):\n",
    "            print(f\"Processing EXCEL file: {file.name}\")\n",
    "            excel_text = extract_text_from_excel(file.name)\n",
    "            if excel_text:\n",
    "                print(f\"Extracted text from EXCEL file: {excel_text[:100]}...\")  # Debug log (first 100 chars)\n",
    "                documents.append(Document(excel_text, metadata={\"source\": file.name}))\n",
    "            else:\n",
    "                print(f\"No text extracted from the EXCEL file: {file.name}\")\n",
    "        elif file.name.endswith(\".docx\"):\n",
    "            print(f\"Processing WORD file: {file.name}\")\n",
    "            word_text = extract_text_from_word(file.name)\n",
    "            if word_text:\n",
    "                print(f\"Extracted text from WORD file: {word_text[:100]}...\")  # Debug log (first 100 chars)\n",
    "                documents.append(Document(word_text, metadata={\"source\": file.name}))\n",
    "            else:\n",
    "                print(f\"No text extracted from the WORD file: {file.name}\")\n",
    "        elif file.name.endswith(\".eml\"):\n",
    "            print(f\"Processing EML file: {file.name}\")\n",
    "            eml_text = extract_text_from_eml(file.name)\n",
    "            if eml_text:\n",
    "                print(f\"Extracted text from EML file: {eml_text[:100]}...\")  # Debug log (first 100 chars)\n",
    "                documents.append(Document(eml_text, metadata={\"source\": file.name}))\n",
    "            else:\n",
    "                print(f\"No text extracted from the EML file: {file.name}\")\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file.name}\")\n",
    "            # Continue processing other files instead of returning\n",
    "            continue\n",
    "\n",
    "    # Process website URLs\n",
    "    for url in urls:\n",
    "        print(f\"Processing website: {url}\")\n",
    "        website_text = extract_text_from_website(url)\n",
    "        if website_text:\n",
    "            print(f\"Extracted text from website: {website_text[:100]}...\")  # Debug log (first 100 chars)\n",
    "            documents.append(Document(website_text, metadata={\"source\": url}))\n",
    "        else:\n",
    "            print(f\"No text extracted from the website: {url}\")\n",
    "\n",
    "    # Ensure that documents are created and provide debug info\n",
    "    if not documents:\n",
    "        print(\"No valid content extracted from the document(s).\")\n",
    "        return \"No valid content extracted from the document(s).\"\n",
    "\n",
    "    print(f\"Document list contains {len(documents)} item(s).\")\n",
    "\n",
    "    # Create Chroma vector database and embed the documents\n",
    "    try:\n",
    "        print(f\"Embedding documents into Chroma: {len(documents)} document(s)\")\n",
    "        global vector_db  # Making vector_db accessible globally\n",
    "\n",
    "        # Generate a unique collection name to prevent reuse\n",
    "        collection_name = f\"collection_{uuid4()}\"\n",
    "\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True),\n",
    "            collection_name=collection_name,\n",
    "            persist_directory=None  # Use in-memory storage to avoid persistence\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating the vector database: {e}\")\n",
    "        return \"Error during vector embedding process.\"\n",
    "\n",
    "    return \"Documents and websites successfully processed and embedded.\"\n",
    "\n",
    "# Function to query details and populate the checklist\n",
    "def populate_checklist(checklist_path):\n",
    "    global vector_db  # Access the global vector database\n",
    "\n",
    "    # Check if vector_db is initialized\n",
    "    if vector_db is None:\n",
    "        return \"Please process the documents first before populating the checklist.\"\n",
    "\n",
    "    local_model = \"llama3.1\"  # LLM model\n",
    "    llm = ChatOllama(model=local_model)  # Initialize the LLM\n",
    "\n",
    "    retriever = vector_db.as_retriever()\n",
    "\n",
    "    # Create the RetrievalQA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=False\n",
    "    )\n",
    "\n",
    "    # Populate the checklist using the chain\n",
    "    try:\n",
    "        wb = load_workbook(checklist_path)\n",
    "        ws = wb.active\n",
    "\n",
    "        for row_idx, row in enumerate(ws.iter_rows(min_row=2, max_col=2, values_only=False), start=2):  # Assume column A has queries and column B will get populated\n",
    "            query_cell = row[0]\n",
    "            if query_cell.value:  # Ensure the query is not empty\n",
    "                # Query the QA model for the specific detail\n",
    "                query_result = qa_chain.run(query_cell.value)\n",
    "                # Populate column B with the result\n",
    "                ws.cell(row=row_idx, column=2).value = query_result\n",
    "\n",
    "        # Save the updated checklist\n",
    "        wb.save(checklist_path)\n",
    "        print(f\"Checklist updated and saved to {checklist_path}\")\n",
    "        return f\"Checklist successfully populated and saved to {checklist_path}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while populating the checklist: {e}\"\n",
    "\n",
    "# Function to handle asking a question about the embedded documents\n",
    "def ask_question(question):\n",
    "    global vector_db  # Access the global vector database\n",
    "\n",
    "    # Check if vector_db is initialized\n",
    "    if vector_db is None:\n",
    "        return \"Please process the documents first before asking a question.\"\n",
    "\n",
    "    local_model = \"llama3.1\"  # LLM model\n",
    "    llm = ChatOllama(model=local_model)  # Initialize the LLM\n",
    "\n",
    "    retriever = vector_db.as_retriever()\n",
    "\n",
    "    # Create the RetrievalQA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=False\n",
    "    )\n",
    "\n",
    "    # Retrieve the answer from the chain\n",
    "    answer = qa_chain.run(question)\n",
    "    return answer\n",
    "\n",
    "# Gradio Layout with CSS for font size and header\n",
    "with gr.Blocks() as iface:\n",
    "    # Display the logo\n",
    "    gr.Image(logo_path, label=\"\", type=\"filepath\", interactive=False)\n",
    "    \n",
    "    # Add header text below the logo\n",
    "    gr.Markdown(\"<h2 style='text-align: center; color: red;'> AI Powered Customer Fulfilment Intelligent Document Processor & Chat Bot </h2>\")\n",
    "    \n",
    "    # Input section for files, website URLs, and checklist path with larger font\n",
    "    with gr.Row():\n",
    "        file_input = gr.Files(\n",
    "            label=\"Upload your documents (PDF, Image, EXCEL, WORD, or EML)\", \n",
    "            file_types=['.jpg', '.jpeg', '.png', '.pdf', '.xlsx', '.xls', '.docx', '.eml'],\n",
    "            elem_id=\"file_input\"\n",
    "        )\n",
    "        url_input = gr.Textbox(label=\"Enter website URLs (separated by commas)\", elem_id=\"url_input\")\n",
    "        checklist_input = gr.Textbox(label=\"Checklist Excel File Path\", elem_id=\"checklist_input\")\n",
    "    \n",
    "    # Button to trigger document processing\n",
    "    process_button = gr.Button(\"Process Documents and Websites\")\n",
    "    \n",
    "    # Output section for document processing result with larger font\n",
    "    result_output = gr.Textbox(label=\"Output\", elem_id=\"result_output\")\n",
    "    \n",
    "    # Input for asking questions about the documents\n",
    "    question_input = gr.Textbox(label=\"Ask a Question About the Documents\", placeholder=\"Type your question here...\")\n",
    "    \n",
    "    # Button to ask a question and display the answer\n",
    "    ask_button = gr.Button(\"Ask Question\")\n",
    "    answer_output = gr.Textbox(label=\"Answer\", interactive=False)\n",
    "    \n",
    "    # Button to process and populate the checklist\n",
    "    populate_button = gr.Button(\"Populate Checklist\")\n",
    "    populate_output = gr.Textbox(label=\"Checklist Status\", interactive=False)\n",
    "\n",
    "    # Trigger the process_documents function on button click\n",
    "    def process_inputs(files, url_text, checklist_path):\n",
    "        # Split the URL text into a list of URLs\n",
    "        urls = [url.strip() for url in url_text.split(\",\") if url.strip()]\n",
    "        return process_documents(files, urls, checklist_path)\n",
    "    \n",
    "    process_button.click(process_inputs, inputs=[file_input, url_input, checklist_input], outputs=result_output)\n",
    "    \n",
    "    # Trigger the ask_question function to retrieve answers based on user's query\n",
    "    ask_button.click(ask_question, inputs=question_input, outputs=answer_output)\n",
    "\n",
    "    # Trigger the populate_checklist function to populate the checklist\n",
    "    populate_button.click(populate_checklist, inputs=checklist_input, outputs=populate_output)\n",
    "\n",
    "# Custom CSS for larger font sizes\n",
    "iface.css = \"\"\"\n",
    "#file_input label, #checklist_input label, #result_output label, #question_input label, #answer_output label, #populate_output label {\n",
    "    font-size: 18px;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55827dd5-5fde-4816-990a-ecb1c13d5bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
