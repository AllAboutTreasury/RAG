{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e875668-5650-4d6a-a0e8-110d20e65e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://bfe80514976527f666.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bfe80514976527f666.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import fitz  # PyMuPDF\n",
    "import cv2\n",
    "from pyzbar.pyzbar import decode\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from docx import Document as DocxDocument\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from uuid import uuid4\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "import difflib\n",
    "import webbrowser\n",
    "\n",
    "# Path to your logo\n",
    "logo_path = r\"C:\\Users\\M\\Desktop\\RAKBANK.png\"\n",
    "\n",
    "# Global variables\n",
    "vector_db = None\n",
    "SUPPORTED_TYPES = ['.pdf', '.jpg', '.jpeg', '.png', '.xlsx', '.xls', '.docx', '.eml', '.txt']\n",
    "QR_SUPPORTED_TYPES = ['.pdf', '.jpg', '.jpeg', '.png']\n",
    "HTML_DIFF_CSS = \"\"\"\n",
    "<style>\n",
    "    table.diff {font-family:Courier; border:medium; width:100%;}\n",
    "    .diff_header {background-color:#e0e0e0}\n",
    "    td.diff_header {text-align:right}\n",
    "    .diff_next {background-color:#c0c0c0}\n",
    "    .diff_add {background-color:#aaffaa}\n",
    "    .diff_chg {background-color:#ffff77}\n",
    "    .diff_sub {background-color:#ffaaaa}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, text, metadata=None):\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "def extract_qr_code(files):\n",
    "    \"\"\"Process multiple files for QR codes and open websites\"\"\"\n",
    "    results = []\n",
    "    opened_urls = set()\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            if file.name.endswith('.pdf'):\n",
    "                pdf_document = fitz.open(file.name)\n",
    "                for page_num in range(len(pdf_document)):\n",
    "                    page = pdf_document.load_page(page_num)\n",
    "                    pix = page.get_pixmap()\n",
    "                    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                    \n",
    "                    open_cv_image = np.array(img)\n",
    "                    open_cv_image = open_cv_image[:, :, ::-1].copy()\n",
    "                    \n",
    "                    decoded_objects = decode(open_cv_image)\n",
    "                    if decoded_objects:\n",
    "                        results.append(f\"\\nüìÑ {file.name} (Page {page_num+1}):\")\n",
    "                        for obj in decoded_objects:\n",
    "                            data = obj.data.decode('utf-8')\n",
    "                            results.append(f\"üîó {data}\")\n",
    "                            if data.startswith(('http://', 'https://')) and data not in opened_urls:\n",
    "                                webbrowser.open(data)\n",
    "                                opened_urls.add(data)\n",
    "                pdf_document.close()\n",
    "            else:\n",
    "                img = Image.open(file.name)\n",
    "                open_cv_image = np.array(img)\n",
    "                open_cv_image = open_cv_image[:, :, ::-1].copy()\n",
    "                decoded_objects = decode(open_cv_image)\n",
    "                \n",
    "                if decoded_objects:\n",
    "                    results.append(f\"\\nüì∑ {file.name}:\")\n",
    "                    for obj in decoded_objects:\n",
    "                        data = obj.data.decode('utf-8')\n",
    "                        results.append(f\"üîó {data}\")\n",
    "                        if data.startswith(('http://', 'https://')) and data not in opened_urls:\n",
    "                            webbrowser.open(data)\n",
    "                            opened_urls.add(data)\n",
    "        \n",
    "        except Exception as e:\n",
    "            results.append(f\"\\n‚ùå Error processing {file.name}: {str(e)}\")\n",
    "    \n",
    "    if not results:\n",
    "        return \"No QR codes found in any documents\"\n",
    "    \n",
    "    if opened_urls:\n",
    "        results.append(\"\\n\\nüåê Opened URLs:\")\n",
    "        results.extend([f\"- {url}\" for url in opened_urls])\n",
    "    \n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        return pytesseract.image_to_string(img)\n",
    "    except Exception as e:\n",
    "        return f\"Image error: {str(e)}\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        for page in pdf_document:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"PDF error: {str(e)}\"\n",
    "\n",
    "def extract_text_from_excel(excel_path):\n",
    "    try:\n",
    "        wb = load_workbook(excel_path, data_only=True)\n",
    "        return '\\n'.join(\n",
    "            ' '.join(str(cell) for cell in row)\n",
    "            for sheet in wb\n",
    "            for row in sheet.iter_rows(values_only=True)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"Excel error: {str(e)}\"\n",
    "\n",
    "def extract_text_from_word(word_path):\n",
    "    try:\n",
    "        doc = DocxDocument(word_path)\n",
    "        return '\\n'.join(para.text for para in doc.paragraphs)\n",
    "    except Exception as e:\n",
    "        return f\"Word error: {str(e)}\"\n",
    "\n",
    "def extract_text_from_eml(eml_path):\n",
    "    try:\n",
    "        with open(eml_path, 'rb') as fp:\n",
    "            msg = BytesParser(policy=policy.default).parse(fp)\n",
    "        text_parts = []\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                text_parts.append(part.get_payload(decode=True).decode('utf-8', errors='replace'))\n",
    "        return '\\n'.join(text_parts)\n",
    "    except Exception as e:\n",
    "        return f\"EML error: {str(e)}\"\n",
    "\n",
    "def extract_text_from_file(file):\n",
    "    try:\n",
    "        if file.name.endswith(\".pdf\"):\n",
    "            return extract_text_from_pdf(file.name)\n",
    "        elif file.name.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            return extract_text_from_image(file.name)\n",
    "        elif file.name.endswith((\".xlsx\", \".xls\")):\n",
    "            return extract_text_from_excel(file.name)\n",
    "        elif file.name.endswith(\".docx\"):\n",
    "            return extract_text_from_word(file.name)\n",
    "        elif file.name.endswith(\".eml\"):\n",
    "            return extract_text_from_eml(file.name)\n",
    "        elif file.name.endswith(\".txt\"):\n",
    "            with open(file.name, 'r') as f:\n",
    "                return f.read()\n",
    "        else:\n",
    "            return \"Unsupported file type\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def compare_documents(doc1, doc2):\n",
    "    if not doc1 or not doc2:\n",
    "        return \"Upload both documents\"\n",
    "    \n",
    "    try:\n",
    "        text1 = extract_text_from_file(doc1)\n",
    "        text2 = extract_text_from_file(doc2)\n",
    "        \n",
    "        differ = difflib.HtmlDiff()\n",
    "        return HTML_DIFF_CSS + differ.make_file(\n",
    "            text1.splitlines(), \n",
    "            text2.splitlines(),\n",
    "            fromdesc=\"Document 1\",\n",
    "            todesc=\"Document 2\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"Comparison failed: {str(e)}\"\n",
    "\n",
    "def process_documents(files, urls, checklist_path):\n",
    "    documents = []\n",
    "    \n",
    "    for file in files:\n",
    "        text = extract_text_from_file(file)\n",
    "        if text:\n",
    "            documents.append(Document(text, metadata={\"source\": file.name}))\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                text = soup.get_text(separator=' ')\n",
    "                documents.append(Document(text, metadata={\"source\": url}))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    if not documents:\n",
    "        return \"No content found\"\n",
    "    \n",
    "    try:\n",
    "        global vector_db\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "            collection_name=f\"collection_{uuid4()}\",\n",
    "            persist_directory=None\n",
    "        )\n",
    "        return \"Processing successful\"\n",
    "    except Exception as e:\n",
    "        return f\"Processing error: {str(e)}\"\n",
    "\n",
    "def ask_question(question):\n",
    "    if not vector_db:\n",
    "        return \"Process documents first\"\n",
    "    \n",
    "    try:\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=ChatOllama(model=\"llama3.1\"),\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=vector_db.as_retriever()\n",
    "        )\n",
    "        return qa_chain.run(question)\n",
    "    except Exception as e:\n",
    "        return f\"QA error: {str(e)}\"\n",
    "\n",
    "def populate_checklist(checklist_path):\n",
    "    if not vector_db:\n",
    "        return \"Process documents first\"\n",
    "    \n",
    "    try:\n",
    "        wb = load_workbook(checklist_path)\n",
    "        ws = wb.active\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=ChatOllama(model=\"llama3.1\"),\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=vector_db.as_retriever()\n",
    "        )\n",
    "        for row_idx in range(2, ws.max_row + 1):\n",
    "            question = ws.cell(row=row_idx, column=1).value\n",
    "            if question:\n",
    "                answer = qa_chain.run(question)\n",
    "                ws.cell(row=row_idx, column=2).value = answer\n",
    "        wb.save(checklist_path)\n",
    "        return \"Checklist populated\"\n",
    "    except Exception as e:\n",
    "        return f\"Checklist error: {str(e)}\"\n",
    "\n",
    "with gr.Blocks(title=\"Document Processor\", css=\".gradio-container {max-width: 1200px !important}\") as iface:\n",
    "    gr.Image(logo_path, label=\"\", height=100)\n",
    "    gr.Markdown(\"<h1 style='text-align: center'>AI Document Processor</h1>\")\n",
    "    \n",
    "    # QR Code Section\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            gr.Markdown(\"## QR Code Scanner\")\n",
    "            qr_upload = gr.Files(label=\"Upload Documents\", \n",
    "                                file_types=QR_SUPPORTED_TYPES,\n",
    "                                file_count=\"multiple\")\n",
    "            qr_button = gr.Button(\"Scan All QR Codes\", variant=\"primary\")\n",
    "            qr_output = gr.Textbox(label=\"Scan Results\", lines=8)\n",
    "    \n",
    "    # Document Comparison\n",
    "    with gr.Accordion(\"Document Comparison Tools\", open=False):\n",
    "        with gr.Row():\n",
    "            doc1 = gr.File(label=\"First Document\")\n",
    "            doc2 = gr.File(label=\"Second Document\")\n",
    "        compare_btn = gr.Button(\"Compare Documents\")\n",
    "        diff_output = gr.HTML()\n",
    "    \n",
    "    # Main Processing\n",
    "    with gr.Row():\n",
    "        files = gr.Files(label=\"Upload Documents\", \n",
    "                       file_types=SUPPORTED_TYPES,\n",
    "                       file_count=\"multiple\")\n",
    "        urls = gr.Textbox(label=\"Website URLs (comma-separated)\",\n",
    "                        placeholder=\"Enter URLs separated by commas\")\n",
    "        checklist = gr.Textbox(label=\"Checklist Path\",\n",
    "                             placeholder=\"Path to Excel checklist file\")\n",
    "    \n",
    "    # Actions\n",
    "    with gr.Row():\n",
    "        process_btn = gr.Button(\"Process Documents\", variant=\"primary\")\n",
    "        with gr.Column(scale=2):\n",
    "            question_input = gr.Textbox(label=\"Ask a Question\",\n",
    "                                      placeholder=\"Type your question here...\")\n",
    "            ask_btn = gr.Button(\"Get Answer\")\n",
    "        checklist_btn = gr.Button(\"Populate Checklist\", variant=\"secondary\")\n",
    "    \n",
    "    # Outputs\n",
    "    with gr.Row():\n",
    "        status = gr.Textbox(label=\"Processing Status\")\n",
    "        answer = gr.Textbox(label=\"Answer\")\n",
    "        checklist_status = gr.Textbox(label=\"Checklist Status\")\n",
    "    \n",
    "    # Event Handling\n",
    "    qr_button.click(extract_qr_code, qr_upload, qr_output)\n",
    "    compare_btn.click(compare_documents, [doc1, doc2], diff_output)\n",
    "    process_btn.click(\n",
    "        lambda f, u, c: process_documents(f, [x.strip() for x in u.split(\",\") if x.strip()], c),\n",
    "        [files, urls, checklist],\n",
    "        status\n",
    "    )\n",
    "    ask_btn.click(ask_question, question_input, answer)\n",
    "    checklist_btn.click(populate_checklist, checklist, checklist_status)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc5ef2-01f6-4980-8a05-027b4970fd17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
